{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pneumonia recognition",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkJq1OQc2SK7",
        "colab_type": "text"
      },
      "source": [
        "# Pneumonia detection\n",
        "\n",
        "This dataset contains images of x-rays with pneumonia infections and without. The task of the following neural network is to classify pneumonia infections correctly.\n",
        "The dataset comes from https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPP6jc_aRIyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKR910JI5vyN",
        "colab_type": "text"
      },
      "source": [
        "## Connect the dataset\n",
        "Download the dataset from kaggle to your local machine and [run this notebook locally](https://research.google.com/colaboratory/local-runtimes.html) This can take time(for me round about 30 seconds). Don't forget to edit the DATADIR to your local path."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6FkbFoa5lby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Command to start local runtime: jupyter notebook \\ --NotebookApp.allow_origin='https://colab.research.google.com' \\ --port=8888 \\ --NotebookApp.port_retries=0\n",
        "\n",
        "CATEGORIES = [\"NORMAL\", \"PNEUMONIA\"]\n",
        "IMG_SIZE = 300\n",
        "TRAINING_DATA = []\n",
        "ROOT_DIR = \"C:/Users/carmesin/Desktop/transfer_learning/\"\n",
        "CHECKPOINT_DIR = ROOT_DIR + \"checkpoints/checkpoint/checkpoints.ckpt\"  # Path to checkpoint that will be created during training\n",
        "TRAIN_DIR = ROOT_DIR + \"dataset/train\"  # Path to your train folder\n",
        "TEST_DIR = ROOT_DIR + \"dataset/test\"  # Path to your test folder\n",
        "VALIDATION_DIR = ROOT_DIR + \"dataset/val\"  # Path to your validation folder\n",
        "\n",
        "# Helper funtion for loading the data in\n",
        "def load_data(DATASOURCE):\n",
        "  image_arr = []\n",
        "  label_arr = []\n",
        "  for category in CATEGORIES:\n",
        "      path = os.path.join(DATASOURCE, category)\n",
        "      label = CATEGORIES.index(category)\n",
        "      for img in os.listdir(path)[:10]:\n",
        "        image = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
        "        image = image / 255 # Normalize the data\n",
        "        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "        image_arr.append(image)\n",
        "        label_arr.append(label)\n",
        "  image_arr = np.array(image_arr).reshape(-1, IMG_SIZE, IMG_SIZE, 1) # Convert to numpy array\n",
        "  label_arr = np.array(label_arr).reshape(-1, 1) # Convert to numpy array\n",
        "  return image_arr, label_arr\n",
        "\n",
        "(train_x, train_y) = load_data(TRAIN_DIR)\n",
        "(test_x, test_y) = load_data(TEST_DIR)\n",
        "(validation_x, validation_y) = load_data(VALIDATION_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEzQP9qLK0Iy",
        "colab_type": "text"
      },
      "source": [
        "## Let's look at the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjh1uZSij-q_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Train inputs: \", train_x.shape)\n",
        "print(\"Train labels: \", train_y.shape)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Test inputs: \", test_x.shape)\n",
        "print(\"Test labels: \", test_y.shape)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Validation inputs: \", validation_x.shape)\n",
        "print(\"Validation labels: \", validation_y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oeauBSILEzT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(train_x[0][:, :, 0])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjEM8d9LCMCW",
        "colab_type": "text"
      },
      "source": [
        "## The Network\n",
        "\n",
        "For an image classification task a neural network works best. So first I will flatten the image into a long array of numbers. The last layer forms the results of the hidden layers into a probability distribution using to softmax activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScxtI18yke9v",
        "colab_type": "code",
        "outputId": "954ff171-fb4c-4337-9144-b1003bae8a84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 820
        }
      },
      "source": [
        "def create_classical_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Conv2D(32, [3, 3], activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(tf.keras.layers.Conv2D(64, [3, 3], activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.2))\n",
        "    model.add(tf.keras.layers.Conv2D(128, [3, 3], activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dense(1))\n",
        "    return model\n",
        "\n",
        "\n",
        "CHECKPOINT_DIR = os.path.dirname(CHECKPOINT_DIR)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=CHECKPOINT_DIR,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "model = create_classical_model()\n",
        "model.load_weights(CHECKPOINT_DIR) # Path to stored weights, if you don't have any stored weights you can comment this out\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_x,\n",
        "          train_y,\n",
        "          epochs=1,\n",
        "          batch_size=16,\n",
        "          verbose=1,\n",
        "          shuffle=True,\n",
        "          validation_data=(validation_x, validation_y),\n",
        "          callbacks=[cp_callback])\n",
        "\n",
        "model.evaluate(test_x, test_y)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-8-780f3e9c0faa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_classical_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCHECKPOINT_DIR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n\u001b[0;32m     25\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdadelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[0;32m    232\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[0;32m    233\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[1;32m--> 234\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[0;32m   1213\u001b[0m           'first, then load the weights.')\n\u001b[0;32m   1214\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assert_weights_created\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1215\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1216\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;34m'layer_names'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m'model_weights'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_weights'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[0;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m                                swmr=swmr)\n\u001b[0m\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'C:/Users/carmesin/Desktop', errno = 13, error message = 'Permission denied', flags = 0, o_flags = 0)"
          ]
        }
      ]
    }
  ]
}